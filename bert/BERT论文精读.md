PPT1：

大家好!今天我要和大家分享一篇NLP领域具有里程碑意义的论文—BERT。

Transformer架构有效利用了自注意力机制，BERT为预训练-微调的模型范式奠定了基础，推动了NLP领域的快速发展。

之所以分享这篇文章是因为近期我在学习RAG相关知识的时候，深刻感受到了BERT在底层的作用。

PPT2：

首先，介绍一下今天分享的整体框架。我会从以下八个方面来展开讲解。

第一部分引言，我带大家简要回顾一下NLP的发展历程。

接下来，我会详细解读BERT论文的核心内容，包括其创新的模型结构和预训练、微调方法等内容。

第三部分，我将分享用Pytorch进行BERT核心部分的代码复现，帮助大家更直观地理解BERT的实现细节。

第四部分，我们会分析BERT的创新点、它在NLP领域产生的深远影响，以及目前的主要应用场景。

第五和第六部分，我们将客观地探讨BERT的局限性，以及它的后续发展和改进方向。

第七部分，我准备了一些思考题，希望能够引发大家的思考和讨论。

最后，对整个讲解内容进行总结和归纳，以及BERT和智能问答方向结合的思考。

通过这样的安排，希望能够帮助大家全面地理解BERT的技术创新与实践价值。

PPT3：

首先让我们来看一下自然语言处理中语言模型的发展历史。这张时间线展示了从1949年到2022年NLP领域的重要里程碑。

在2001年之前，我们称之为'Before ML era'，也就是机器学习之前的时代。那时主要依靠统计方法和规则来处理自然语言。

2008年是一个转折点，开始尝试将多任务学习引入到自然语言处理中。

在2008年，自然语言处理（NLP）开始引入多任务学习的概念，这意味着一个模型可以在同一训练过程中同时学习多个相关任务。这样做的优势在于，模型可以共享不同任务的知识，提高学习效率和泛化能力。例如，模型可能同时学习句子分类、情感分析和命名实体识别等任务，从而在自然语言理解上更加全面。这一创新为后续NLP模型的开发奠定了重要基础。

到了2013年，随着Word2vec和N-gram模型的出现，我们第一次能够以向量的形式来表示单词的语义信息。

2014年是深度学习在NLP领域大放异彩的一年，RNN和LSTM的广泛应用让模型能够处理序列数据，这是一个质的飞跃。2015年，注意力机制的提出又为NLP带来了新的突破。

2017年是又一个重要的转折点，Google提出的Transformer架构彻底改变了NLP的技术范式。而在2018年，也就是我们今天要重点讨论的BERT模型诞生了。

在BERT之后，我们看到了GPT-3、T5等大语言模型的出现。每一个模型都在不断推动着NLP技术的边界。

当然在此之后就是ChatGPT的问世所引发的AI浪潮，开源模型和专有模型，迭代更新，在Scaling Law法则的加持下，通用性愈发强大。

"Scaling Law" 指的是，模型的性能（例如在各种NLP任务上的表现）会随着模型规模（参数数量）、数据集大小和计算资源的增加而提升。 通常，这种提升并非线性，而是遵循某种幂级关系。

当然近期也有传言，这个Scaling Law法则遇到了一些障碍，高质量数据稀缺等问题使得模型性能提升速度变缓。

我个人还是相信Scaling Law法则的，这个世界的数据还远没有被消耗殆尽。一些复杂问题，单纯的Scaling并不能解决，但可以结合思维链（Chain Of Thought）来改善。

以上就是语言模型的发展历史。

PPT4：

接下来让我们聚焦到2018年，这一年Google发布了BERT模型。BERT是Bidirectional Encoder Representations from Transformers的缩写，正如这个名字所暗示的，它具有几个重要的特征：

首先是'Bidirectional'，也就是双向的特性。与之前的模型相比，BERT能够同时利用上下文的信息来理解文本，这是一个革命性的突破。

其次，'Encoder'说明它是一个编码器结构，能够将输入的文本转化为深层的语义表示。

再次，'Representations'表明它学习到的是文本的表征，这些表征可以被用于各种下游任务。

最后，'from Transformers'告诉我们它是基于Transformer架构，继承了自注意力机制的优势。

这些特性的结合，使得BERT在发布后立即引起了学术界和工业界的广泛关注。它不仅在11个NLP任务上取得了当时最好的成绩，更重要的是开创了'预训练+微调'这一范式。

PPT5：

接下来进行BERT论文的讲解。大家可以结合QQ群里面的论文PDF来听我的讲解。

PPT6：

首先看一下论文的摘要。

讲了BERT和之前语言模型的区别（如ELMo、GPT）

GPT考虑的是单向，用左边的上下文信息去预测未来

ELMo用了一个基于RNN的架构（双向LSTM来理解上下文），BERT用的是Transformer。

ELMo应用到下游任务，需要对架构进行一些调整。而BERT应用到下游任务，不需要从根本上改变下游任务的模型架构，可以进行微调或添加特定任务的层。

BERT效果强大，在当时11个任务中取得SOTA水平，里面提及到的基准测试，简单介绍一下，后续也会提到。

四个基准测试：

1. GLUE (General Language Understanding Evaluation)
- 通用语言理解评估
- 是一个综合性语言理解基准测试集合,包含多个子任务如自然语言推理、相似度计算、情感分析等
- 用于全面评估模型的语言理解能力

2. MultiNLI (Multi-Genre Natural Language Inference)

- 多类型自然语言推理
- 给定两个句子,判断它们之间的关系是蕴含、矛盾还是中性
- 数据来自多种文本类型(新闻、小说、口语等),更全面地测试模型泛化能力

3. SQuAD v1.1 (Stanford Question Answering Dataset)

- 斯坦福问答数据集(第1.1版)
- 给定一段文本和相关问题,模型需要从文本中找出答案
- 答案一定存在于给定文本中

4. SQuAD v2.0

- 斯坦福问答数据集(第2.0版)
- 是 v1.1 的升级版,增加了"无法回答"的情况
- 更贴近实际应用场景,因为并非所有问题都能从给定文本中找到答案

总结：

摘要讲了BERT和之前工作的关联性，以及BERT的不同之处。
BERT效果很好，在当时11个NLP任务上达到SOTA水平。

PPT7：

接下来介绍论文的第一部分介绍

在自然语言处理领域，任务可以分为两大类：句子层面的任务，比如判断两个句子的关系、分析句子的情感;还有词元层面的任务，如命名实体识别和问答系统。这些任务都需要深入理解语言的语义。

在使用预训练模型时，主要有两种策略。一种是像ELMo这样基于特征的方法，它将预训练得到的表示作为额外的特征输入到专门设计的任务架构中。另一种是像GPT这样基于微调的方法，它通过简单调整预训练模型来适应下游任务。这两种方法在预训练阶段都使用单向的语言模型。

但是单向模型存在局限性。以GPT为例，它只能看到前面的词来预测下一个词，这对很多任务来说都不够理想。特别是在问答这样的任务中，我们需要同时理解问题的上下文。ELMo采用的是双向LSTM架构，但是这种双向是用了两个独立的单向语言模型，其输出组合在一起，并不是真正的双向。

这就是提出了BERT的原因。它有两个关键的预训练任务：一个是掩码语言模型，随机遮住一些词让模型预测【完形填空】，这样模型就能学习到双向的上下文信息；另一个是预测两个句子是否相邻，这让模型能够理解句子间的关系。

文章的贡献：

- 展示了双向信息的重要性
- 有较好的预训练模型的话，不用对特定的任务进行特定模型的改动

BERT是第一个基于微调的模型，在一系列的NLP任务上【包括了句子层面和词元层面】都取得了最好的成绩。

- 代码、模型开源

PPT8：

接着看一下论文的结论。

BERT的工作实际上是站在巨人的肩膀上。ELMo证明了双向架构的价值，而GPT展示了Transformer的强大。在此基础上，BERT的创新在于将这两者的优势结合起来，并做了关键的改进。

具体来说，BERT没有像之前语言模型那样预测下一个词，而是采用了类似完形填空的方式进行预训练。这个改变看似简单，却让BERT能够真正实现深度的双向建模，让模型可以同时看到并利用左右两侧的上下文信息。

这种方法的强大之处在于，同一个预训练模型可以适应各种不同的自然语言处理任务。更重要的是，即使是那些训练数据很少的任务，也能从这个预训练模型中受益。

BERT的工作不仅证明了双向建模的重要性，还展示了一个统一的预训练模型如何能够服务于广泛的NLP应用。

PPT9：

接着介绍论文的第二部分：相关工作

在无监督预训练领域,我们看到了两个主要的技术路线。 

第一个是基于特征的方法,从早期的词嵌入发展到后来的ELMo,这条路线的核心是让模型能够提取上下文相关的特征表示。

 第二个是基于微调的方法,以GPT为代表。这类方法的优势在于通过预训练获得了良好的初始化参数,因此在下游任务时不需要从头开始学习。

除了无监督预训练外,利用有标注数据进行迁移学习也是一个重要的技术路线,比如使用自然语言推理或机器翻译任务的数据。

但这种监督式的迁移学习方法在NLP领域遇到了两个主要挑战:一是不同任务之间的差异较大,迁移的效果受限;二是高质量的标注数据获取成本高,规模往往较小。

包括BERT在内的一系列工作证明,在NLP领域,使用大规模无标注数据进行预训练,要优于在相对较小的标注数据集上进行迁移学习。

有趣的是,这个发现正在影响其他领域。比如在计算机视觉领域,研究者们也开始探索使用无标注数据进行预训练,发现相比传统的在ImageNet这样的标注数据集上训练,使用无标注数据预训练能够获得更好的视觉表示。

这个趋势给我们的启示是,充分利用大规模无标注数据的潜力,是未来人工智能发展的一个重要方向。当然这个启示在当下已经应验了。

PPT10：

接着向大家介绍论文第三部分—BERT模型。

BERT的训练过程分为两个阶段:预训练和微调。在预训练阶段,模型在大规模无标注数据上进行自监督学习,完成一系列预训练任务。在微调阶段,我们首先用预训练得到的参数初始化BERT模型,然后在下游任务的标注数据上对所有参数进行微调。值得注意的是,尽管不同任务的模型参数都是从同一个预训练模型初始化得到的,但它们在微调后会演变成不同的模型。后续有一张图示清晰地展示了这一点。

BERT的一个显著特点是它在不同任务上使用了统一的模型架构。预训练模型和下游任务模型在结构上几乎没有区别。这种设计使得BERT可以非常方便地适应各种不同的任务。

BERT的模型架构用一句话概括，就是多层双向的Transformer编码器堆叠。BERT采用了Transformer中的编码器部分，不同于其他仅从左到右或从右到左的语言模型，BERT是双向的，这意味着它可以同时考虑到一个词前后所有的上下文信息，从而更好地理解词语的含义。

在这个架构中，有三个关键参数决定了模型的规模和复杂度。第一个参数是L，也就是Transformer块的个数。简单来说，这个参数决定了模型有多少层，层数越多，模型的表征能力也就越强。

第二个参数是H，它表示隐藏层的大小，也就是每一层的宽度。这个参数越大，模型能够捕捉的信息也就越丰富。

第三个参数是A，指的是自注意力机制中多头的个数。多头注意力让模型可以从多个角度去看待输入信息，能够增强模型对不同特征的理解能力。

主要汇报了两种尺寸的模型结果:BERTBASE是一个中等大小的模型,它有12层Transformer块,隐藏层维度是768,自注意力头数是12,总参数量是1.1亿;BERTLARGE是一个大模型,它有24层Transformer块,隐藏层维度是1024,自注意力头数是16,总参数量是3.4亿。

选择BERTBASE的规模,是为了与OpenAI GPT模型保持一致,便于比较。但是,BERT Transformer和GPT Transformer在注意力机制上有一个关键区别:BERT使用了双向自注意力,而GPT使用了受限的自注意力,每个token只能关注它左边的上下文。

PPT11：

这页PPT展示的是BERT的最初版本

最初，BERT只有BERT-Base和BERT-Large两个版本，但随着时间推移，研究团队添加了多种变体，以适应不同的计算资源需求，例如BERT-Tiny、BERT-Mini等更小的模型，以及带有“Whole Word Masking”【全词掩码】预处理的模型。这些模型主要用于更高效的推理和应用场景。

Whole Word Masking：

产生背景：
- 在BERT最初的预训练中,是随机对WordPiece tokens进行掩码
- 这可能导致一个完整的词被部分掩码,使得预测任务过于简单

例子：

```
原始文本:
the man jumped up , put his basket on phil ##am ##mon ' s head

原始掩码方式:
[MASK] man [MASK] up , put his [MASK] on phil [MASK] ##mon ' s head
(可以看到随机掩码,可能只掩盖词的一部分)

全词掩码方式:
the man [MASK] up , put his basket on [MASK] [MASK] [MASK] ' s head
(对于"philammon"这个词,它的所有片段都被一起掩码了)
```

全词掩码的优点:

- 使模型必须根据上下文预测整个完整的词
- 增加了任务难度,因为不能依赖词的部分信息
- 迫使模型学习更好的语义理解

%% BERT仓库的代码是用TensorFlow实现的。并不建议大家去复现该版的代码。原因有：

1. 框架更迭快速：

- TensorFlow 1.x (BERT原始仓库使用的版本)已经相对较老
- PyTorch已经成为学术界和工业界的主流
- Hugging Face的Transformers库已经成为事实上的标准

2. Hugging Face的优势：

- 提供标准化、稳定且优化的实现
- 大量预训练模型可直接使用
- 完善的文档和社区支持
- 支持多个深度学习框架(PyTorch/TensorFlow)
- 持续更新维护，跟进最新研究进展

因此，在2024年的当下，学习BERT重点把握以下几点：

- 理解BERT的基本原理和创新点
- 学习如何使用Hugging Face高效开发
- 关注具体任务的解决方案 而不是纠结于复现较老的TensorFlow实现。 %%

PPT12：

接下来介绍的是BERT模型是如何进行预训练的。与之前的一些工作不同,BERT并没有使用传统的从左到右或从右到左的语言模型来做预训练,而是采用了两个新颖的无监督预训练任务。

第一个任务叫做Masked LM。顾名思义,就是随机mask掉一些输入的token,然后让模型去预测这些被mask掉的token。

直观上来看,这种深度双向的预训练方式比单向的语言模型更有表达能力。但如果直接采用双向语言模型,每个词都能看到自己,这就失去了预测的意义。所以BERT巧妙的引入了mask机制,随机mask掉15%的token,然后预测这些token。

值得一提的是,BERT在mask的时候,有80%的概率会替换成[MASK]标记,10%的概率替换成其他随机词,10%的概率保持不变。这样做是为了缓解预训练阶段引入[MASK]标记而在fine-tuning阶段没有[MASK]的不匹配情况。

第二个预训练任务叫做Next Sentence Prediction,缩写为NSP。许多下游任务如问答、自然语言推理等都要考虑句子对之间的关系,而语言模型无法直接建模这种句子间的关系。所以BERT加入了一个二分类任务,50%的概率选择紧邻的句子对,标签为IsNext;50%的概率随机选择语料库中的句子作为句子对,标签为NotNext。

BERT的预训练过程基本遵循现有的语言模型预训练方法。在预训练语料方面，BERT使用了BooksCorpus（小说数据集包含8亿个单词）和英文维基百科（包含25亿个单词）。对于维基百科的部分，只提取了文本段落，忽略了列表、表格和标题等内容。

使用文档级别的语料库【文本层面的数据集】（而非句子级语料库）非常重要，因为这样可以提取到较长的连续文本序列。这有助于模型更好地学习上下文和句子之间的关系，而不是只处理孤立的句子。

PPT13：

这页PPT展示的是MLM任务。右边例子讲解xxx。

%% 做掩码时，会将词元替换成一个特殊的token—[MASK]，在训练的时候，15%的词对应的是[MASK]，微调的时候没有该目标函数，没有[MASK]，导致预训练、微调的时候，看到的数据有一点点不一样，会带来一点点问题。

差异带来的问题：在预训练中，模型会看到大量包含[MASK]的输入，这有助于模型学习如何预测被遮蔽的词。然而，在实际微调和应用过程中，输入通常不包含[MASK]，因此模型可能会遇到不一致的数据分布，从而影响性能。 %%

解决方法：15%选中为掩码的词，80%替换为掩码符号—[MASK]，10%替换为随机的词元，10%什么都不干，用于做预测。

%% 为什么这样能解决问题？

•解决数据分布差异：这种掩码策略确保模型在预训练时不仅见到[MASK]，还会接触到一些“随机替换”和“保持不变”的情况，增加了模型对不同上下文形式的适应性。这使得模型在微调或推理阶段，即使没有[MASK]标记，也能更准确地理解词语间的关系。

•丰富上下文语境的学习：随机替换和保持不变的策略帮助模型学习更具鲁棒性的语义关联，使得模型对未遮蔽词的上下文关系有更深的理解，不单单依赖[MASK]去学习。 %%

PPT14：

这页PPT展示的是NPS任务。右边例子讲解xxx。

BERT 的下一句预测 (Next Sentence Prediction, NSP) 任务的设计初衷是为了帮助模型理解句子之间的关系，但这项任务在后续的研究中被发现存在一些问题，可能带来负面效果。

- 在此后的研究（论文《Crosslingual language model pretraining》等）中发现，NSP任务可能并不是必要的，消除NSP损失在下游任务的性能上能够与原始BERT持平或略有提高。这可能是由于BERT以单句子为单位输入，模型无法学习到词之间的远程依赖关系。针对这一点，后续的RoBERTa、ALBERT、spanBERT都移去了NSP任务。

在BERT的NSP任务中，模型需要判断两个句子是否连续，但在实际任务中，两个句子之间的连续性往往无法直接代表上下文的逻辑关系。NSP任务可能导致模型学习到的特征偏离真正的语言理解目标，使模型关注一些无关紧要的模式，反而弱化了对句子间语义关联的理解。

一些后续的研究和模型（例如 RoBERTa等）放弃了 NSP 任务，转而采用其他的预训练任务，实验证明，去除 NSP 任务后，这些模型在很多下游任务上的性能反而得到了提升。

PPT15：

这是BERT的预训练过程示意图，讲解xxx。

PPT16：

继续讲解BERT论文的第三部分，刚才讲了BERT的两阶段—预训练和微调。

接下来要讲的是BERT是如何表示输入数据的。

%% 众所周知,我们希望BERT能够处理各种各样的下游任务,这就要求它的输入表示必须足够灵活,能够明确地表示单个句子或者句子对,比如一个问答任务中的问题和答案。

首先解释一下对"句子"和"序列"这两个概念的定义。从语言学的角度来说,一个句子通常是指语法和语义完整的一个单元。但在我们的工作中,"句子"其实指的是文本中任意连续的一段,"序列"才是专门指输入给BERT的token序列,它可以是一个句子,也可以是两个句子的拼接。【将两个句子变为一个序列，BERT使用的是序列对】 %%

使用WordPiece算法【如果一个词在整个序列中出现的概率不大的话，应该将其切开，看子序列，如果子序列是词根，出现概率较大，则只保留子序列，3万词典就能表示比较大的文本】对文本进行分词,建立了一个包含30000个token的词表。

输入序列的第一个token永远是一个特殊的分类token,我们把它记作[CLS]。这个token对应的最后一层的隐藏状态向量,将被用作整个序列的聚合表示,用于分类任务【代表整个序列的一个信息】。

当我们要输入一对句子时,它们首先会被打包成一个单一的序列。但我们需要让模型能够区分哪些token属于句子A,哪些属于句子B。我们采取了两个措施:第一,在两个句子中间插入一个特殊的分隔符[SEP];第二,对于序列中的每一个token,我们都添加了一个学习得到的segment embedding,来指示它属于句子A还是句子B【两个句子合在一起作为一个序列，需要区分】。

最后,对于序列中的每一个token,它的输入表示是三个embedding的和,分别是:这个token本身的embedding、它所属的句子的segment embedding、它在序列中的位置的positional embedding。我们把这三个embedding加起来,就得到了每个token的最终输入表示。

注意点：transformer中，位置信息是手动构造出来的一个矩阵，在bert中，不管属于哪个句子，还是位置在哪里，对应的向量表示都是学习得来的。

PPT17：

这里我们有一张图示直观地展示了输入表示的构建过程。大家可以看到,最下面一排是positional embedding,中间一排是segment embedding,最上面是token embedding,三者相加,就是每个token的输入表示。

通过这种输入表示方式,BERT可以灵活地处理各种单句和句子对的任务,为它在海量的下游任务上取得优异表现打下了坚实的基础。

PPT18：

BERT的微调过程相对简单，得益于Transformer中的自注意力机制，它可以处理多种下游任务，无论是单文本输入，还是文本对输入。

对于文本对的任务，传统方法通常会先独立编码文本对，再进行双向交叉注意力处理，而BERT通过自注意力机制将这两个阶段统一起来。将文本对拼接后用自注意力机制进行编码，可以自然地引入双向交叉注意力，帮助模型更好地理解两个句子之间的关系。

在每个具体任务中，我们只需将任务特定的输入和输出接口接入BERT模型【设计输入和输出】，然后进行端到端的微调。例如，在输入端，BERT预训练的句子A和句子B可以对应不同的任务类型，比如复述中的句对、蕴涵任务中的假设-前提对、问答任务中的问题-段落对，或者文本分类中的文本-空对。

```
(1) 复述任务（Paraphrase）:
输入: [CLS] 句子A [SEP] 句子B [SEP]
例子: [CLS] 我很喜欢这部电影 [SEP] 这电影真不错 [SEP]
目标: 判断两句话是否表达相同的意思

(2) 自然语言推理(NLI):
输入: [CLS] 前提句 [SEP] 假设句 [SEP]
例子: [CLS] 猫在桌子上睡觉 [SEP] 有动物在桌子上 [SEP]
目标: 判断两句话的逻辑关系(蕴含/矛盾/中性)

(3) 问答任务(QA):
输入: [CLS] 问题 [SEP] 文本段落 [SEP]
例子: [CLS] 作者是谁? [SEP] 这本书是张三写的... [SEP]
目标: 从段落中找出答案位置

(4) 文本分类:
输入: [CLS] 文本 [SEP]
例子: [CLS] 这个产品质量很好 [SEP]
目标: 进行情感分类等
```

输出端的处理则有所不同：对于序列标注或问答任务，使用的是每个token的表示结果，而对于分类任务（如蕴涵或情感分析），使用的是[CLS]标记的表示结果。

```
(1) 分类任务(如复述/NLI/情感分析):
- 使用[CLS]标记的最终向量表示
- [CLS]向量包含了整个输入序列的全局信息
- 接一个全连接层做分类

例如情感分析:
[CLS]向量 -> FC层 -> Softmax -> 积极/消极

(2) 序列标注任务(如NER/词性标注):
- 使用每个token的向量表示
- 每个token都需要一个标签预测
- 通常接一个全连接层预测每个位置的标签

例如命名实体识别:
token1向量 -> FC层 -> B-PER
token2向量 -> FC层 -> I-PER
...

(3) 问答任务:
- 使用所有token的向量表示
- 预测答案的起始和结束位置
- 通常用两个全连接层分别预测起始/结束概率

段落中的每个token:
token向量 -> FC层1 -> 起始位置概率
token向量 -> FC层2 -> 结束位置概率
```

与预训练相比，微调的计算成本较低。即便在单个Cloud TPU上，微调最多只需要一小时，在GPU上也只需几小时。所有实验结果都是基于同一个预训练模型复现的。

PPT19：

这是BERT的总体训练流程图,它清晰地展示了BERT的训练分为两个阶段:预训练和微调。

在预训练阶段,使用大规模无标注语料对BERT进行自监督学习。模型的输入是两个经过遮挡的句子A和B,我们让BERT完成两个任务:一个是Masked LM,即预测被遮挡的单词;另一个是Next Sentence Prediction,即判断句子B是否是句子A的下一句。通过这两个任务,BERT可以学习到语言的基本规律和上下文表征能力。

预训练完成后,我们就进入了微调阶段。对于不同的下游任务,模型的结构保持不变,只是输入和输出会有所不同。

以问答任务为例,输入变成了一个问题和一段上下文,而输出是从上下文中抽取出的答案片段。再比如自然语言推理任务,输入是两个句子,输出是它们之间的推理关系。模型在微调时,所有的参数都会用下游任务的有标注数据进行训练,但初始参数都是来自同一个预训练模型。这也是BERT能够适应多种任务的关键所在。

%% 相同的模型架构在微调时被应用于不同的下游任务，如MNLI【MNLI (Multi-Genre Natural Language Inference) 是多类型自然语言推理任务】、NER、SQuAD等。

BERT模型被用于MNLI、NER和SQuAD等不同任务，它们都使用相同的基础BERT架构和预训练参数作为起点，但每个任务都有其特定的输出层和微调目标。
 %%
 
图中我们还可以看到,[CLS]是一个特殊的分类符号,它会被添加到每个输入样本的开头;[SEP]是另一个特殊分隔符,用于隔离句子对中的两个句子。它们在预训练和微调中都扮演着重要的角色。

PPT20：

我们来思考一下，BERT模型的微调和目前的LLM微调之间有何差异。

这边有一张NLP范式进化路线图。可以看到BERT模型的微调处于第三阶段，而目前的LLM微调处于第四阶段。如果你试验过微调LLM的话，你就会发现LLM微调数据的格式通常由输入和预期输出的数据组成，每条训练数据包含一个输入（Prompt）及其对应的预期输出。

以问答任务为例，来感受一下两者微调之间的差异。

```
bert训练数据格式
{ "context": "北京是中国的首都，位于华北平原的北部，是一座有着3000多年建城史、800多年建都史的历史文化名城。北京是中国的政治、文化、国际交往中心和科技创新中心，2008年在这里成功举办了第29届夏季奥林匹克运动会。", "question": "中国的首都是哪座城市？", "answer": { "text": "北京", "start_position": 0,  # 答案在原文中的起始位置 "end_position": 2  # 答案在原文中的结束位置 } }

LLM训练数据格式
{ "prompt": "根据下面这段话回答问题：\n\n北京是中国的首都，位于华北平原的北部，是一座有着3000多年建城史、800多年建都史的历史文化名城。北京是中国的政治、文化、国际交往中心和科技创新中心，2008年在这里成功举办了第29届夏季奥林匹克运动会。\n\n问题：中国的首都是哪座城市？", "completion": "中国的首都是北京。" }
```

bert用于微调问答任务，只适用于抽取式问答，答案在上下文中有的，如果答案需要总结或者没有，就难以应对。

这也就表明了在做问答方面研究时，如果需要用到LLM微调，最好是有标准答案的场景，没有标准答案的场景，还是用LLM结合RAG较好【毕竟LLM的知识有限且存在幻觉/或者说是胡说八道】。

举例子：科学事实(如"光速是多少?")有标准答案，适合LLM微调；实时新闻问答(如"最近的中东局势如何?")，不适合LLM微调。

PPT21：

这页PPT是BERT的概览。

BERT模型最初有两个版本：BERT-Base和BERT-Large，分别适应不同的任务和计算需求。BERT能够处理较长的输入上下文，这使它在许多自然语言处理任务中表现出色。它的预训练使用了大规模的数据集，包括维基百科和BooksCorpus【小说数据集】。

BERT的目标是支持多任务学习，在训练过程中使用了TPU，能够同时支持句子级别和词级别的任务。同时，BERT可以轻松微调来适应不同的下游任务，使得在各类NLP任务中表现优异。

PPT22～PPT26：

接下来介绍论文的第四部分—实验。我也进行了这四个实验，只不过微调BERT时的学习率等超参数和原始论文不同。最后得到的评测指标和论文中所述差不多。

首先,在GLUE（General Language Understanding Evaluation，通用语言理解评估）基准测试中,BERT的基础版（BERT BASE）和大版（BERT LARGE）在所有任务上都显著超过了其他系统，分别比之前的最先进水平提高了4.5%和7.0%的平均准确率。其中在MNLI、QQP、QNLI等大规模数据集上进步尤其明显。这表明BERT学到的语言表征可以有效迁移到多种不同的NLP任务中。

GLUE中的典型任务：

```
(1) MNLI (Multi-Genre Natural Language Inference)
前提：这家餐厅的服务很糟糕
假设：服务员态度很好
关系：矛盾(Contradiction)

(2) QQP (Quora Question Pairs)
问题1：如何学习Python编程？
问题2：入门Python的最佳方法是什么？
任务：判断这两个问题是否等价（是）

(3) QNLI (Question Natural Language Inference)
问题：谁发明了电灯泡？
文本：爱迪生在1879年发明了实用的电灯泡。
任务：判断文本是否包含问题的答案（是）

(4)
MRPC (Microsoft Research Paraphrase Corpus) 是一个用于句子对分类的自然语言处理任务，其目标是判断两个句子是否具有语义等价性（即是否表达了相同的意思）。

示例任务

输入句子对：

• Sentence 1: “He said the food was great.”
• Sentence 2: “He mentioned that the meals were excellent.”

任务目标：
• 输出标签：1 （表示这两个句子是语义等价的）
```

接下来,将BERT应用于SQuAD1.1（Stanford Question Answering Dataset，斯坦福问答数据集）,评估它在阅读理解任务上的表现。系统采用了适度的数据增强策略,先在TriviaQA数据集【大规模的阅读理解数据集】上进行微调,然后再在SQuAD上微调。实验结果非常惊艳。即使不使用TriviaQA数据集,模型F1值【精确率和召回率的调和平均】也只会下降0.1到0.4,依然以巨大优势超过所有已发表的模型。可见语言预训练模型能够显著提升阅读理解任务的性能。【对于每一个词源，判断是否是答案的开头，答案的结尾】

SQuAD 1.1（问答必有答案）例子：

```
文章片段：2008年北京奥运会开幕式于8月8日晚上8点在国家体育场举行。
问题：北京奥运会开幕式是什么时候举行的？
答案：8月8日晚上8点
```

紧接着还测试了在问答场景更复杂的SQuAD2.0数据集，不同于SQuAD1.1,这个数据集允许给定段落中没有答案存在,这使得任务更加贴近真实的问答场景。BERT在这个任务上也比之前的模型优秀,再次展示了它强大的语义理解能力。

SQuAD 2.0（问答可能无答案）例子：

```
文章片段：2008年北京奥运会开幕式于8月8日晚上8点在国家体育场举行。
问题1：开幕式在哪里举行的？
答案：国家体育场

问题2：闭幕式是什么时候举行的？
答案：No Answer（文中没有提到）
```

最后,在SWAG数据集（Situations With Adversarial Generations，对抗生成情景数据集）上评估了BERT在常识推理方面的表现，SWAG是一个面向常识推理的数据集。任务的目标是从四个选项中选出与给定句子最相符的后续句子。更令人惊讶的是,它的性能甚至超过了人类专家。由于SWAG中的句子需要根据日常知识来判断,这说明模型通过预训练学会了很多常识性的内容,具备了一定的推理能力。

SWAG（常识推理选择）例子：

```
上下文：妈妈把蛋糕放进烤箱。
四个选项：
A. 她等待蛋糕烤熟（✓正确）
B. 她把蛋糕扔进垃圾桶
C. 她把蛋糕放在阳光下
D. 她把蛋糕放进冰箱

任务：选择最合理的后续情节
```

从这些基准测试可以看出,BERT模型通过两阶段的学习,即大规模语料上的预训练和具体任务上的微调,习得了强有力的通用语言表征。

PPT27：

接下来分享BERT模型上做的一些消融实验,以更好地理解模型不同部分的相对重要性。

消融实验部分一共有3小节组成。

第一节为预训练任务的效果，使用和BERT Base模型完全一样的预训练数据、fine-tuning方案和超参数,评估了两种预训练目标的效果:

1、不做NSP:也就是只用MLM预训练的双向语言模型,没有next sentence prediction任务。

2、从左到右语言模型+不做NSP:这其实就是GPT模型的预训练方式,只是我们用了更大的数据集、输入表示以及fine-tuning方案。

实验发现,去掉NSP任务对QNLI(Question-answering Natural Language Inference)、MNLI(Multi-Genre Natural Language Inference)和SQuAD的性能有影响。

把双向表示换成从左到右的表示后,在所有任务上效果进一步降低,尤其是MRPC和SQuAD任务。

这直观上也很好理解,因为从左到右的模型在做SQuAD这样的任务时,每个token只能看到左边的上下文。即使我们在上面加了一个随机初始化的BiLSTM来增强这个从左到右的模型,效果虽然有所提升,但还是比不上预训练的双向模型。

第二节为模型大小的影响，训练了层数、隐层大小和attention head数目不同的一系列BERT模型,其它超参数和训练过程则完全一样。结果表明,更大的模型在所有数据集上都能带来准确率的提升,即使是训练样本只有3600个的MRPC任务。

在机器翻译、语言建模等大规模任务上,增大模型规模可以持续带来性能提升, 证明了一点在大量预训练的前提下,极大的模型规模能给很小规模的任务带来巨大提升。

PPT28：

BERT有两种应用方式：微调(Fine-tuning)和特征提取(Feature-based)。第三节通过在命名实体识识别(NER)任务上的对比实验，探讨了这两种方法的效果差异。

%% 1. 两种方法的具体实现：
微调方法：
- 加载预训练的BERT模型
- 添加一个简单的分类层
- 在下游任务上联合优化所有参数（包括BERT参数）
特征提取方法：
- 将BERT当作特征提取器，参数固定不变
- 提取BERT不同层的激活值作为特征
- 将这些特征输入到一个新的双层BiLSTM模型
- 只训练BiLSTM和分类层的参数
2. 实验设计细节：
- 使用区分大小写的WordPiece分词
- 考虑最大文档上下文
- 采用每个token的第一个sub-token表示 %%
- 最佳特征提取方案：拼接BERT最上面4层的表示
3. 实验结果与结论：
- 两种方法都取得了很好的效果
- 但特征提取方法的F1值比微调方法低0.3个百分点
- 这表明：虽然BERT作为特征提取器也能获得不错的效果，但如果条件允许，还是应该优先使用微调方法

这个实验很好地说明了：尽管BERT在两种使用方式下都表现出色，但微调方法能够更充分地发挥模型的性能，是更优的选择。这也成为了后来使用预训练语言模型的主流范式。

PPT29：

这是一张Bert模型结构图。

我们以输入序列 [CLS] I AM FINE [SEP] 为例，更清晰地讲解一下 BERT 的工作流程以及每一层的线性投影的作用。

1. 输入嵌入:
- 首先，输入序列中的每个词（包括 [CLS] 和 [SEP]）都会被转换成对应的词嵌入向量。
- 同时，由于这里只有一个句子，段嵌入会为所有词分配相同的段向量
- 位置嵌入会根据词在序列中的位置，为每个词分配不同的位置向量。
- 最后，将词嵌入、段嵌入和位置嵌入三个向量相加，得到每个词的最终输入嵌入。
2. Transformer 编码器层 (x12): BERT 会将输入嵌入依次送入 12 个 Transformer 编码器层。每个编码器层都进行相同的操作，但参数不同。让我们以其中一层为例进行说明：
- %% 多头自注意力机制: 这是 BERT 的核心。 %%
- 线性投影 (Keys, Queries, Values): 进入自注意力机制后，每个词的输入嵌入都会经过三个不同的线性变换，分别生成 Key、Query 和 Value 向量。 这些线性变换的权重是可以学习的参数，不同的“头”会有不同的线性变换矩阵，从而关注不同的信息。
- 注意力计算: 每个词的 Query 向量会与所有词（包括自身）的 Key 向量进行点积运算，然后通过 Softmax 函数得到注意力权重。这些权重代表了当前词与其他词的相关程度。
- 加权求和: 将所有词的 Value 向量按照对应的注意力权重进行加权求和，得到当前词的上下文表示。 这使得每个词的表示都融合了与其相关的其他词的信息。
- 多头拼接: 由于有多个“头”，每个“头”都会进行上述的注意力计算，得到不同的上下文表示。将这些表示拼接在一起。
- 线性投影 (输出): 最后，将拼接后的向量再进行一次线性变换，得到最终的输出。
- Add & Norm: 将多头自注意力的输出与该层的输入（即上一层的输出或初始输入嵌入）相加，然后进行层归一化，有助于稳定训练和防止梯度消失/爆炸。
- 前馈网络层: 对每个词的向量进行两次线性变换，中间夹杂一个非线性激活函数 (例如 ReLU)。 这进一步增强了模型的表示能力。
- Add & Norm: 与自注意力机制之后的操作相同，将前馈网络的输出与该层的输入相加，然后进行层归一化。
3. 输出: 经过 11 层 Transformer 编码器层后，最终得到每个词的上下文表示。
- [CLS] 输出: [CLS] 标记对应的输出向量可以被视为整个输入序列的表示，通常用于分类任务。
- 其他词输出: 其他词的输出向量可以用于其他任务，例如命名实体识别。

%% 关于线性投影的补充说明：

每一层的线性投影都有各自的作用：

- 自注意力机制中的线性投影 (K, Q, V): 将输入嵌入映射到不同的空间，使模型能够学习不同的注意力模式。
- 自注意力机制输出的线性投影: 将多个“头”的输出整合到一个向量中。
- 前馈网络层中的线性投影: 增加模型的非线性表达能力。

这些线性投影的权重都是模型在训练过程中学习得到的，使得 BERT 能够根据不同的任务和数据学习到最佳的表示方式。 %%

总而言之，BERT 通过多层 Transformer 编码器和自注意力机制，能够捕捉到输入序列中丰富的上下文信息，从而在各种自然语言处理任务中取得优异的性能。

PPT30：

BERT的实现主要包含以下几个关键组件:

首先,在模型初始化阶段,我们定义了几个核心模块:

1、嵌入层(Embedding):负责将输入的词转换为密集的向量表示,包含词嵌入、位置嵌入和段嵌入。

2、Transformer编码器层(EncoderLayer):模型的核心,由多个自注意力层和前馈网络层组成。

3、分类头部:包含全连接层、归一化层和激活函数,用于下游任务。

在前向传播过程中,数据的流动路径是这样的:

1、输入首先通过嵌入层,获得词的向量表示。

2、然后经过多层Transformer编码器,每一层都包含:

- 自注意力机制,捕获序列中的长距离依赖关系
- 前馈神经网络,增强模型的表达能力

3、最后通过分类层得到最终输出。

在实际研究和实验中，通常不需要手动搭建BERT模型。直接调用huggingface的transformer库加载预训练模型，接着在下游任务上进行微调即可。【比如分类任务，在 BERT上面增加了一个分类头（classifier），包含 dropout 层和一个线性层】

除非研究重点是改进BERT的架构本身，否则使用Hugging Face这样成熟的库是更明智的选择，可以让研究者把重点放在实际问题的解决上。

PPT31～34：

```
用SQuAD 1.1数据集微调BERT-base-uncased：

SQuAD 1.1数据集只包含可回答的问题，所有问题都假设在上下文中有答案。
模型倾向于选择看起来与问题最相关的文本片段，而不会验证答案是否真的可以从文本中推断出来。
对于没有答案的问题：模型会强行从文本中选取一个最相关的片段作为答案，即使答案并不正确。
对于需要总结的问题：由于是抽取式问答，只能直接抽取原文中的片段，无法进行文本生成或总结。


在 SQuAD 2.0 上微调的模型能够预测答案的起始和结束位置，同时能够处理“无答案”的情况。你需要从模型输出中提取起始和结束位置的 logits。
将预测的起始和结束 logits 与“无答案”分数进行比较。如果起始和结束 logits 之和低于“无答案” logits，那么模型会判断问题在上下文中没有答案。

“无答案”分数就是模型预测答案的起始位置和结束位置都在一开始 [CLS] 标记上的分数之和。


用CMRC2018数据集微调BERT-base-chinese
出现了一个神奇的现象，微调的数据集中，没有无答案的条目，但是微调好后的模型，面对上下文没有答案的情形，直接输出空字符串【预测答案的开始位置和结束位置为0，即预测的答案为[CLS]】

用Chinese-SQuAD 2.0微调BERT-base-chinese
由于数据集的不平衡，导致微调后的BERT在不可回答问题上表现更好（eval_NoAns_exact = 76.6526）
而在可回答问题上表现较差（eval_HasAns_exact = 36.4069）
```

PPT35：

主要创新点:

1. 双向特性(Bidirectional)
- 首次提出深度双向语言模型的预训练方法
- 通过masked language model(MLM)任务实现双向上下文的融合
- 相比之前单向模型(如GPT)或浅层双向模型(如ELMo)效果更好

2. 预训练任务设计
- Masked LM (MLM): 随机mask 15%的词进行预测,迫使模型学习双向上下文
- Next Sentence Prediction (NSP): 预测两个句子是否为连续句子,学习句子关系

3. 统一的模型架构
- 预训练和微调使用相同的Transformer架构
- 只需添加简单的输出层就可以用于不同下游任务
- 最小化了任务特定的架构设计需求

重要影响:

1. 性能突破
- 在11项NLP任务上取得SOTA结果
- GLUE分数提升7.7%,MultiNLI提升4.6%
- SQuAD v1.1提升1.5%,v2.0提升5.1%
2. 架构创新
- 开创了预训练+微调范式
- 推动了双向语言模型的发展
- 影响了后续大量模型的设计(RoBERTa、ALBERT等)

主要应用:

1. 分类任务
- 情感分析
- 自然语言推理
- 问题匹配等

2. 序列标注
- 命名实体识别
- 词性标注等

3. 问答系统
- 阅读理解
- 问答匹配

4. 其他任务
- 语义相似度计算
- 文本生成等

BERT的出现推动了预训练语言模型的研究和应用,为NLP领域带来了范式转变,是里程碑式的工作。

PPT36、37：

这页PPT展示了BERT在下游任务，如分类任务、问答任务、命名实体识别任务上的应用。


PPT38：

BERT的局限性有：

1. 计算复杂度和资源需求
- 自注意力机制导致序列长度的二次方复杂度 O(n²)
- 需要大量计算资源进行预训练和微调
- 模型参数量庞大(BERT-base: 110M, BERT-large: 340M)

2. 输入长度限制
- 最大序列长度限制在512个token
- 难以处理长文档和跨文档任务
- 无法有效处理超出序列限制的长距离依赖

3. 预训练和微调的不一致性
- [MASK]标记在预训练中出现但实际应用中不存在
- NSP(下一句预测)任务的有效性受到后续研究质疑

4. 架构约束
- 双向特性使其不适合生成任务【BERT可以看到双向上下文。在生成任务中，我们需要模型一个词一个词地生成，每次生成时只能依赖已经生成的内容（左边的上下文），而不能依赖未来的内容（右边的上下文）。这就是为什么自回归模型（如GPT）更适合生成任务，生成时右边的内容还不存在】

%% 自回归（Autoregressive）在语言模型中指的是：模型根据已经生成的内容（历史信息）来预测下一个token的过程。

- 固定的位置编码限制了灵活性
- 层间信息混合可能不是最优的 %%

这些局限性推动了后续模型的改进。

PPT39：

BERT之后出现了多个重要的改进模型，包括RoBERTa、ALBERT等，这些模型在预训练和微调阶段都取得了显著的性能提升。这标志着自然语言处理进入了"预训练模型时代"。

RoBERTa是BERT的优化变体，保持了BERT的基础架构，对BERT进行了精细优化和改进。【建立在 BERT 的基础上，修改了关键的超参数，删除了下一句话预训练目标，并使用更大的小批量和学习率进行训练】

%% 主要改进

训练方法优化：
- 移除了原BERT中的下一句预测(NSP)任务
- 采用动态掩码策略，每个训练轮次使用不同的掩码模式
- 使用更大的批次规模和更长的训练时间

数据处理提升：
- 扩大了训练数据集，从BERT的16GB提升到超过160GB的文本数据
- 使用更大的字节对编码(BPE)词汇表，从BERT的30k增加到50k
- 采用基于字节的BPE编码，更好地处理Unicode字符

虽然RoBERTa的性能优于BERT，但这种改进是以更高的计算成本为代价的。它需要更多的计算资源和更长的训练时间。
 %%
 
DistilBERT 也是 BERT 的一个较小的、更快、更便宜、更轻的变体，它使用了知识蒸馏技术。

知识蒸馏 是一种将大型模型（教师模型）的知识迁移到小型模型（学生模型）的技术。在 DistilBERT 中，BERT 就是教师模型，而 DistilBERT 是学生模型。

D%% istilBERT 的训练过程：

1. 训练 BERT 教师模型: 首先，使用大量数据训练一个标准的 BERT 模型。
2. 用 BERT 的知识训练 DistilBERT 学生模型: 然后，使用训练好的 BERT 模型来指导 DistilBERT 的训练。DistilBERT 不仅学习训练数据的原始标签，还学习 BERT 模型对输入数据的预测概率分布 (也称为“软标签”)。
3. 优化 DistilBERT: 此外，DistilBERT 还使用了一些优化技巧，例如移除 token-type embeddings 和 pooler 层，以及使用更小的隐藏层维度。

DistilBERT 的优势:

- 更小的模型尺寸: DistilBERT 比 BERT 小 40%，参数量更少。
- 更快的推理速度: DistilBERT 比 BERT 快 60%，推理效率更高。
- 较高的性能: DistilBERT 保留了 BERT 97% 的性能，性能损失很小。

DistilBERT 与 BERT 的主要区别:

- 模型大小: DistilBERT 比 BERT 更小，参数更少。
- 训练方法: DistilBERT 使用知识蒸馏技术进行训练，而 BERT 使用标准的预训练方法。
- 架构: DistilBERT 简化了 BERT 的架构，例如移除了一些层和嵌入。

DistilBERT 的应用:

DistilBERT 适用于各种 NLP 任务，例如：

- 文本分类
- 情感分析
- 问答系统
- 命名实体识别
 %%
总而言之，DistilBERT 是一个高效的 BERT 变体，它在保持良好性能的同时，显著减小了模型尺寸和提高了推理速度。这使得 DistilBERT 成为资源有限环境下的理想选择。


ALBERT (A Lite BERT) 是 Google 研发的一种 BERT 的后续变体，旨在解决 BERT 的一些局限性。它在以下方面进行了改进：

1. 减少参数数量:
- 词嵌入分解: BERT 将词嵌入维度与隐藏层维度设为相同大小。ALBERT 将它们分离，先将词嵌入映射到较低维度的空间，再映射到隐藏层维度。这大大减少了词嵌入的参数量。
- 跨层参数共享: BERT 对每个 Transformer 层都有独立的参数。ALBERT 在不同层之间共享参数，进一步减少了参数数量。

2. 提升训练速度:

- 更少的参数: 由于参数数量减少，ALBERT 的训练速度比 BERT 更快。
- 更高的数据吞吐量: 由于计算和通信开销降低，ALBERT 可以更快地处理数据。

3. 改进预训练任务:

- 句子顺序预测 (SOP): BERT 使用下一句预测 (NSP) 任务来学习句子之间的关系。ALBERT 发现 NSP 任务效果不佳，将其替换为 SOP 任务，要求模型判断两个句子的顺序是否正确，这有助于模型更好地理解句子之间的连贯性。

%% ALBERT 的优势:
- 更小的模型尺寸: ALBERT 的模型尺寸比 BERT 小得多，更容易部署到资源有限的设备上。
- 更快的训练速度: ALBERT 的训练速度比 BERT 更快，可以节省训练时间和成本。
- 更好的性能: 在许多 NLP 任务中，ALBERT 的性能都优于 BERT。

ALBERT 的特殊之处:

- ALBERT 的核心思想是通过减少参数数量和改进预训练任务来提高 BERT 的效率和性能。
- ALBERT 证明了在保持性能的同时，大幅减少模型参数是可能的。
- ALBERT 为后续的轻量级语言模型的发展奠定了基础。 %%

总而言之，ALBERT 是 BERT 的一个重要改进，它在保持性能的同时，显著提高了效率。这使得 ALBERT 成为许多 NLP 应用的理想选择，尤其是在资源有限的环境中。

Sentence-BERT 的工作流程：

1. 输入: 将句子输入到 Sentence-BERT 模型中。
2. 编码: Sentence-BERT 使用 BERT 作为编码器，将句子编码成固定长度的向量表示。
3. 池化: 对 BERT 输出的隐藏状态进行池化操作 (例如平均池化或最大池化)，得到句子的最终嵌入向量。
4. 相似度计算: 使用余弦相似度等指标计算句子嵌入向量之间的相似度。

%% Sentence-BERT 的优势：

- 高效: Sentence-BERT 可以高效地生成句子嵌入，并快速计算句子之间的相似度。
- 高质量: Sentence-BERT 生成的句子嵌入能够很好地捕捉句子的语义信息，从而提高相似度计算的准确性。
- 易用: Sentence-BERT 提供了简单易用的 API，可以方便地集成到各种 NLP 应用中。

应用场景:

Sentence-BERT 可以应用于各种需要计算句子相似度的场景，例如：
- 语义搜索: 根据用户查询的语义找到相关的文档或信息。
- 文本聚类: 将语义相似的文本聚类在一起。
- 问答系统: 找到与问题语义最匹配的答案。
- 推荐系统: 推荐语义相似的物品或内容 %%。

总而言之，Sentence-BERT 是一种高效且易用的工具，可以用于生成高质量的句子嵌入并计算句子之间的语义相似度。它在各种 NLP 应用中都有着广泛的应用前景。

当然还有很多很多BERT的变体，以及在变体的基础上迭代产生的新模型。

PPT40：

BERT的后续发展的模型种类丰富。

PPT41：

1、BERT的两种预训练任务:
(1) Masked Language Model (MLM):
- 随机mask掉15%的词进行预测
- 帮助模型学习双向上下文的语义信息和词之间的关系
- 增强模型的语言理解能力

(2) Next Sentence Prediction (NSP):
- 预测两个句子是否是连续的句子对
- 帮助模型理解句子之间的关系
- 对问答和自然语言推理等下游任务有帮助

2、MLM损失函数计算只考虑被mask的tokens(15%中的80%),以及被随机替换的tokens(15%中的10%)和保持不变的tokens(15%中的10%)。即只有这15%的tokens参与损失计算。
备注：损失函数实际上是在比较模型的预测结果与这些位置原本的正确单词。

3、实现损失函数时通过mask矩阵来控制：

```
# 创建一个mask矩阵,标记需要预测的位置为1,其他位置为0  
pred_mask = (masked_tokens != 0)

# 只计算mask位置的损失

loss = loss_fn(predictions[pred_mask], labels[pred_mask])
```

举例：

```
# 假设一个简单的序列示例
原始序列：      [101, 2002, 3012, 4023, 102]  # 101和102是特殊token
masked_tokens： [101, 0,    3012, 0,    102]  # 0表示这个位置被mask了

# 生成的pred_mask将是：
pred_mask：     [False, True, False, True, False]

使用mask矩阵过滤损失计算：
loss = loss_fn(predictions[pred_mask], labels[pred_mask])
```  

predictions[pred_mask]：只选择需要预测位置的模型输出

labels[pred_mask]：只选择对应位置的真实标签

然后只计算这些位置的损失

4、BERT的三种Embedding(Token、Position、Segment)直接相加的原因:

- 简单高效,避免复杂的融合操作
- 三种Embedding本身就是在同一向量空间
- 通过后续的Self-attention层可以学习到它们之间的交互关系

5、BERT的优缺点:

优点:
- 双向语言建模能力强
- 预训练-微调范式简单有效
- 通用性强,适用多种下游任务
- 充分利用无标注数据

可以学习到丰富的语言知识：
- 语法结构
- 语义关系
- 上下文依赖
- 领域知识

缺点:
- 计算资源消耗大
- 预训练时[MASK]标记在微调时不出现,存在预训练-微调差异
- 输入长度限制(最大512)

原始的 BERT 模型有一个固定的输入序列长度限制，最大为 512 个标记（tokens）。这意味着 BERT 一次只能处理最多 512 个标记的文本。

%% 为什么会有这个限制？

- 位置嵌入: BERT 使用位置嵌入来表示标记在序列中的位置信息。在原始的 BERT 模型中，位置嵌入的大小是固定的，最多支持 512 个位置。
- 计算复杂度: 处理更长的序列会增加计算复杂度和内存需求。 %%

- 不适合生成任务
  
6、针对BERT缺点的改进模型:

- RoBERTa: 优化预训练方案
- ALBERT: 参数共享减少资源消耗
%% - XLNet: 自回归预训练避免[MASK]标记
- Longformer: 处理长文本 %%

7、BERT用于生成任务的方法:

- 使用BERT作为编码器,配合解码器（将BERT作为生成模型的组件）

8、BERT在LLM时代的价值:

- 在特定场景仍具优势(如短文本理解)
- 计算资源要求低,部署成本小
- 可解释性更好
- 作为基础研究的重要参考
- 在一些垂直领域仍有应用价值

所以BERT并未过时,而是在合适的场景下仍然是一个很好的选择。

RAG中的reranker到现在仍然是BERT居多。

技术优势

- 擅长文本相似度计算
- 对上下文语义理解深入

%% 相比于新一代的 LLM，例如 GPT 系列，BERT 有一些局限性，例如：

- 生成能力较弱: BERT 主要用于理解任务，其生成能力不如 GPT 等自回归模型。
- 输入长度限制: BERT 对输入文本长度有限制，通常为 512 个标记。
- 计算资源需求高: BERT 模型较大，需要较多的计算资源进行训练和推理。

然而，BERT 仍然具有以下优势：

- 强大的编码能力: BERT 在各种自然语言理解任务中表现出色，其编码能力仍然很强大。
- 广泛的应用: BERT 已经被广泛应用于各种 NLP 应用，例如搜索引擎、问答系统、情感分析等。
- 丰富的变体和工具: BERT 拥有丰富的变体和工具，例如 ALBERT、DistilBERT 等，可以满足不同的需求。

BERT 的现状:

- 不再是 SOTA: 在许多 NLP 任务中，BERT 已经被 newer 的 LLM 超越。
- 应用于特定场景: BERT 仍然适用于一些特定场景，例如对输入长度有限制、对推理速度要求较高、对理解能力要求较高的场景。
- 作为 LLM 的基础: BERT 的架构和预训练方法仍然是许多 LLM 的基础。

总而言之，BERT 虽然不再是 NLP 领域的 SOTA，但它仍然是一个重要的模型，并在许多场景中发挥着作用。 %%

PPT42：其余思考题

PPT43：

BERT是近年来NLP领域最重要的突破之一。

它通过引入MLM和NSP两种预训练任务,率先实现了基于Transformer的真正双向语言表征学习。同时,BERT提出了"预训练-微调"的两阶段迁移学习范式,使得下游任务可以用相同的网络架构实现端到端训练,极大简化了模型开发流程。

BERT的理论创新主要体现在:双向建模能力、统一框架适配能力、预训练-微调范式的推广。实践影响主要体现在:刷新11项NLP任务SOTA、催生大量后续预训练模型、推动工业界广泛应用落地。

当然,BERT也存在一些局限性,如预训练代价大等。后续工作如RoBERTa、ALBERT等从不同角度对BERT进行了改进和扩展。

BERT并没有过时，目前大多在在很底层的地方发挥基础作用。

PPT44：

就比如我目前正在进行的问答系统中。

文本的Embedding和Reranker用到的智源研究院开源的模型，模型的底层还是会有BERT的影子。

有关BERT精读的分享就是这些，谢谢大家。